{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, in this notebook, I have written down the code for multiple regression with Gradient Descent minimisation. The first cell contains the entire code to create the appropriate data frame which is explained in detail in my other notebook 'Numpy,Pandas and CSV'. Do check that out first. What I also aim to do is give you the general overview of how to approach any ML problem. This includes:\n",
    "1. Loading up the dataset\n",
    "    -  Using pandas, numpy as described in the other notebook\n",
    "2. Plotting against the various independants\n",
    "3. Remodelling the dataset for faster and better fitting of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ones, zeros, arange\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "L=pd.read_csv('machine.csv',header=None)\n",
    "\n",
    "L=L.drop(L.columns[[1]],axis=1)\n",
    "L=L[L[0]!='adviser']\n",
    "L=L[L[0]!='microdata']\n",
    "L=L[L[0]!='sratus']\n",
    "\n",
    "Y=L[0].unique()\n",
    "i=1\n",
    "for y in Y:\n",
    "    L[0].replace(y,i,inplace=True)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As good practice, we should always visualise the data we are dealing with in any ML problem. This helps us identify whether a feature affects a classification/regression or whether PCA etc. is needed. Matplotlib is a library that allows for visualisation of data. The syntax is shown in the first cell. We can create a scatter plot of the Machine Cycle Time and Estimated Relative performance. Various commands are displayed in the cell below. The name the functionality I guess ;). A very important thing to note is that we can keep adding plots and it will all show up when we use the show() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmcXFWd9/HPN02ARpZmiQwJhLAZh0VZWpYHRx0EwqIS\nGVRUhnXEBXFBI2REAYUBjeM2+KhsAiMiewC3EFkflUQTAgTQSFiENMhmwtpASH7PH+dUctN0VVd1\nd3VVdX/fr1e9+t5zb937q1vV9at77rnnKCIwMzOr1qhGB2BmZq3FicPMzGrixGFmZjVx4jAzs5o4\ncZiZWU2cOMzMrCZOHFaRpI9KumGI9/mwpL2Hcp9WG0m3SPqPIdqXJP1E0mJJfxyKfVplThxDRNJH\nJM2R9IKkxyX9WtLbGx1XXyLikojYt9FxVKsVko6kkPSkpNUKZaNzWfRYd5Kk2yQ9L+kpSbdKep+k\n/8yfpRckvSxpWWH+3qF/VXX1dmAfYNOI2LXRwZgTx5CQdALwXeC/gI2B8cD/BQ5qZFx9KX6x2aBb\nDOxfmN8/l60g6RDgCuBiYFPSZ+erwHsj4r8iYu2IWBv4BHB7aT4ithuSVzAE8mdwc+DhiHixn8+3\nwRYRftTxAawHvAB8oMI6a5ASy2P58V1gjbzsXcAi4EvAk8DjwGTgAOCvwD+A/yxs61TgSuAy4Hng\nDuCtheUnAQ/kZfcB7y8sOxL4PfAd4Bng9Fz2u7xcedmTwHPAfGD7wuu8GHgK+BtwMjCqsN3fAd8i\nfTk+BOxf4Xg8DEzN8S0GfgKsWVj+HuBOYAnwB+Atufx/geVAdz7mXwIuAr6Ql48DAjguz2+Vj9+o\nStvNy8YCV+XX9xDwmR7H/PL8+p8H7gU6K7y+yMfnikLZlcCX07/kimP9CDClis/Yiveoj/WuAP4O\nPAvcBmxXWHYh8APgl/k1zAa2KizfB/hLfu7ZwK3Af5TZz6lU/gz2dSyvBH6aP2MfB14GluX39LS8\n3seAhfn9uw4Y2+P4HgfcDzxUKPtULnse+Hp+//+Q93M5sHped33gFzm+xXl608L2b8nP/33e1g3A\nRoXlb8/bXQI8ChxZ+D//Vn5fnwB+BLQ3+juqX99rjQ5guD+A/YDXgNUqrPM1YBbwRmBM/tB9PS97\nV37+V4HR+R/mKeBnwDrAdqQvyi3y+qcCS4FD8vpfzP+co/PyD+R/3FHAh4AXgU3ysiPzvo4HVgPa\nWTVxTALmAh2kL7Z/Ljz3YuDaHNMEUlI7prDdpTn2NuCTpASpMsfjYeAeYDNgg/wPenpethMpce2W\nt3VEXn+NwnP3LmzraOD6PP0RUtK8rLDs2r62m4/V3PwerA5sCTwITCoc85dJybwNOBOYVeH9DmB7\n0pdHB+mL6olcFnmdN+f1tqjiM7biPepjvaPz+1P6oXJnYdmFpB8Lu+b3/hLg53nZRqQvyNJn6vP5\nc1IpcfT6GazyWC4l/TgaRY/PYF5nL+BpYOf8Wv4HuK3H8Z2ZPzvthbJrgXVJ/zOvADfm/a9H+pFy\nRF53Q+DfgLXy8boCmF7Y/i35c/SmHN8twFl52eb5WH04v94NgR3zsu+QktwGebvXA2c2+juqX99r\njQ5guD+AjwJ/72OdB4ADCvOTSKfmkBJHN9CW59fJ/wS7FdafC0zO06dS+NLK/3yPA/9SZt93Agfl\n6SOBR3osX/FPm/9h/wrsTv6VnsvbgFeBbQtlHwduKWxjYWHZWvk1/FOZmB4GPlGYPwB4IE//kJxU\nC8sXAO8sPLeYOLYi/WocRfqF93FgUV52EXBCX9slJZOex2Uq8JPCMf9tYdm2QHeF9zuArYHzcjyf\nAM7NZZHX2TOvt2a57fT2HtXwuezI218vz18InNfjmP8lTx/e4zMl0llwpcTR62ewymN5W6XXB5wP\nfLMwvzYp2UwoHN+9ejnme/b4nzmxMP/fwHfLvJ4dgcWF+VuAkwvznwJ+U3gt1/SyDZF+pBXP4vYg\nnxG12sP1f/X3DLCRpNUi4rUy64wlVe+U/C2XrdhGRCzL09357xOF5d2kf56SR0sTEbFc0qLS9iQd\nDpxAOisgP2+j3p7bU0TcJOlsUpXG5pKuJv2abCf9uur5GsYV5v9e2M5Lkkr7LqcYR/F4bA4cIen4\nwvLVWfV4FWN+QNKLpH/+fyFVMRwjaSIpKXy/iu0uA8ZKWlJY1gb8v95eH/ASsGYf7zmks7QzSV8q\nJ/ZY9kz+uwnp1/qASGoDziCdcY4hVelBeu+fzdM9X0Pp/RnLqp+pkFT2c5KV+wwGfR/LvrY9llT9\nVdr+C5KeIX3eHq6wjZ7/Mz3n/wlA0lqks4P9SGeDAOtIaiv8H5Y7VpuRfgj2NIb0g2lu/uxDet/b\nen2FTc4Xx+vvdtJp8eQK6zxG+uIqGZ/L+muz0oSkUaQLq49J2pz0y/bTwIYR0UGqElLhuVFpwxHx\n/YjYhfSr+k3AFFK1wdJeXkPXYLwGVj0ejwJnRERH4bFWRFxaIf5bSdUmq0dEV54/gvSlcGcV232U\n9MuwuGydiDhgAK8P0pflJqSL3r/rsWxB3u+/DXAfJR8hNcbYm1Q1MyGXq9wTCh5n1c+UWPX96U2v\nn0GqO5YVP4P0+H+R9AZSlVDx89bXNir5AjCRdFa/LvCO0q6qeO6jpLPcnp4mJaftCq97vUiNG1qO\nE0edRcSzpPrcH0iaLGmt3PRyf0nfzKtdCpwsaYykjfL6Px3AbneRdHBuUfI5UuKaBbyB9A/1FICk\no0j16lWR9DZJu0kaTTrtfhlYnn+FXQ6cIWmdnKBOGOBrOE7SppI2IF00viyXnwt8IschSW+QdKCk\ndfLyJ0j11kW3kpLlbXn+ljz/u8IvyErb/SPwvKQTJbVLapO0vaS3DeD1leqk3gu8L0/3XHYC8BVJ\nR0laV9IoSW+XdE4/drcO6XPwDOmX73/V8NxfAtsVPlOfIf86r6DcZ3AwjuWlwFGSdpS0Rn4tsyPi\n4Rq2Uck6pC/5Jfnzd0oNz70E2FvSByWtJmlDSTtGxHLSZ+w7kt4IIGmcpEmDFPOQcuIYAhHx36Qv\ngZNJX9qPkr64pudVTgfmAHeTWirdkcv661rShe/FwL8DB0fE0oi4j1SXezvpC3YH0oXnaq1L+vAv\nJlUfPQNMy8uOJyWTB0m/nn8GXDCA1/AzUmuVB0mn/qcDRMQc0kX2s3McC0l14CVnkpLwEklfzGW3\nkr4MSonjd6Qvz9J8xe3m5PIeUnXXQ6Rfj+eRfrkPSETcGxG93ncREVeS3sejSb+ynyAdh2v7sauL\nSe9ZF+lC8KwaYnyaVMV1Fuk934a+PzflPoMDPpYR8VvgK6SWWY+TfuEfWu3zq/BdUvXr06Tj9Jsa\nYnuEdH3oC6QWX3cCb82LTyR9rmZJeg74LenMpuWoxw8da3GSTgW2jojDGh2LjUz+DA5/PuMwM7Oa\nOHGYmVlNXFVlZmY18RmHmZnVZFjeALjRRhvFhAkTGh2GmVlLmTt37tMRMaav9YZl4pgwYQJz5sxp\ndBhmZi1F0t/6XstVVWZmViMnDjMzq4kTh5mZ1cSJw8zMauLEYWZmNRmWrarMzEaa6fO6mDZjAY8t\n6WZsRztTJk1k8k7j+n5iPzhxmJm1uOnzuph69Xy6l6ZRArqWdDP16vkAdUkerqoyM2tx02YsWJE0\nSrqXLmPajAV12Z8Th5lZi3tsSXdN5QPlxGFm1uLGdrTXVD5QThxmZi1uyqSJtI9uW6WsfXQbUybV\nZ4DBuiUOSRdIelLSPYWyaZL+IuluSddI6igsmyppoaQFxXF4Je2XyxZKOqle8ZqZtarJO43jzIN3\nYFxHOwLGdbRz5sE71K1VVd3G45D0DuAF4OKI2D6X7QvcFBGvSfoGQEScKGlb0gD0uwJjSWPxvilv\n6q/APsAi4E/Ah/PY2WV1dnaGOzk0M6uNpLkR0dnXenU744iI20iDtRfLboiI1/LsLGDTPH0Q8POI\neCUiHiIN6L5rfiyMiAcj4lXg53ldMzNrkEZe4zga+HWeHgc8Wli2KJeVK38dScdKmiNpzlNPPVWH\ncM3MDBqUOCR9GXgNuGSwthkR50REZ0R0jhnT5zgkZmbWT0N+57ikI4H3AO+OlRdYuoDNCqttmsuo\nUG5mZg0wpGcckvYDvgS8LyJeKiy6DjhU0hqStgC2Af5Iuhi+jaQtJK0OHJrXNTOzBqnbGYekS4F3\nARtJWgScAkwF1gBmSgKYFRGfiIh7JV0O3EeqwjouIpbl7XwamAG0ARdExL31itnMzPpWt+a4jeTm\nuGZmtWt4c1wzMxuenDjMzKwmThxmZlYTJw4zM6uJE4eZmdXEicPMzGrixGFmZjVx4jAzs5o4cZiZ\nWU36TBySNpZ0vqRf5/ltJR1T/9DMzKwZVXPGcSGpr6ixef6vwOfqFZCZmTW3ahLHRhFxObAcII/g\nt6yuUZmZWdOqJnG8KGlDIAAk7Q48W9eozMysaVXTrfoJpDEwtpL0e2AMcEhdozIzs6bVZ+KIiDsk\nvROYCAhYEBFL6x6ZmZk1pWpaVR0HrB0R90bEPcDakj5V/9DMzKwZVXON42MRsaQ0ExGLgY/VLyQz\nM2tm1SSONuVxXgEktQGr1y8kMzNrZtVcHP8NcJmkH+f5j+cyMzMbgapJHCeSksUn8/xM4Ly6RWRm\nZk2tmlZVy4Ef5oeZmY1wfSYOSXsCpwKb5/UFRERsWd/QzMysGVVTVXU+8HlgLu5qxMxsxKsmcTwb\nEb+ueyRmZtYSqmmOe7OkaZL2kLRz6dHXkyRdIOlJSfcUyjaQNFPS/fnv+rlckr4vaaGku4vbl3RE\nXv9+SUf061WamdmgqeaMY7f8t7NQFsBefTzvQuBs4OJC2UnAjRFxlqST8vyJwP7ANvmxG+lC/G6S\nNgBOyfsOYK6k6/JNiGZm1gDVtKr61/5sOCJukzShR/FBwLvy9EXALaTEcRBwcUQEMEtSh6RN8roz\nI+IfAJJmAvsBl/YnJjMzG7hqzjiQdCCwHbBmqSwivtaP/W0cEY/n6b8DG+fpccCjhfUW5bJy5b3F\neCxwLMD48eP7EZrVYvq8LqbNWMBjS7oZ29HOlEkTmbxTr2+NmQ0z1XRy+CPgQ8DxpKa4HyA1zR2Q\nfHYRA91OYXvnRERnRHSOGTNmsDZrvZg+r4upV8+na0k3AXQt6Wbq1fOZPq+r0aGZ2RCo5uL4/4mI\nw4HFEXEasAfwpn7u74lcBUX++2Qu7wI2K6y3aS4rV24NNG3GArqXrtoyu3vpMqbNWNCgiMxsKFWT\nOLrz35ckjQWWApv0c3/XAaWWUUcA1xbKD8+tq3YnNQF+nDTW+b6S1s8tsPbNZdZAjy3prqnczIaX\naq5x/EJSBzANuINUvdRnX1WSLiVd3N5I0iJS66izgMslHQP8DfhgXv1XwAHAQuAl4CiAiPiHpK8D\nf8rrfa10odwaZ2xHO129JImxHe0NiMbMhprSpYYqV5bWANaMiKYec7yzszPmzJnT6DCGrdI1jmJ1\nVfvoNs48eAdfIDdrYZLmRkRnX+tV01dVG3AgMKG0viQi4tsDDdJaUyk5uFWV2chUTVXV9cDLwHxg\neX3DsVYxeadxThRmI1Q1iWPTiHhL3SMxM7OWUE2rql9L2rfukZiZWUuo5oxjFnCNpFGkpril8TjW\nrWtk1tR857jZyFVN4vg26aa/+VFLEywbtnq2qirdOQ44eZiNANVUVT0K3OOkYSW+c9xsZKvmjONB\n4BZJvwZeKRW6Oe7I5TvHzUa2ahLHQ/mxen7YCOc7x81GtoqJI9/8t05EfHGI4rEWMGXSxF7vHJ8y\naWIDozKzoVIxcUTEMkl7DlUw1hp857jZyFZNVdWdkq4DrgBeLBVGxNV1i8qanu8cNxu5qkkcawLP\nsOoY4wE4cZiZjUDVjDl+1FAEYmZmraGaoWM3lXSNpCfz4ypJmw5FcGZm1nyquQHwJ6QR+sbmx/W5\nzMzMRqBqEseYiPhJRLyWHxcCY+ocl5mZNalqEsczkg6T1JYfh5EulpuZ2QhUTeI4mjQ2+N+Bx4FD\nyGOCm5nZyFO2VZWkb0TEicCuEfG+IYzJzMyaWKUzjgMkCZg6VMGYmVnzq3Qfx2+AxcDakp4jD+CE\nB3IyMxvRyp5xRMSUiOgAfhkR60bEOsW/QxijmZk1kYoXx3PvuIOeJCR9XtK9ku6RdKmkNSVtIWm2\npIWSLpO0el53jTy/MC+fMNjxmJlZ9SomjohYBiyXtN5g7VDSOOAzQGdEbA+0AYcC3wC+ExFbk6rI\njslPOQZYnMu/k9czM7MGqaaTwxeA+ZJmsmrvuJ8Z4H7bJS0F1iI1890L+EhefhFwKvBD4KA8DXAl\ncLYkeShbM7PGqCZxXM0g9oQbEV2SvgU8AnQDNwBzgSUR8VpebRFQ6rN7HGnccyLiNUnPAhsCTxe3\nK+lY4FiA8ePHD1a4ZmbWQzW9414kqR0YHxELBrpDSeuTziK2AJaQxvnYb6DbjYhzgHMAOjs7fTZi\nZlYn1fSO+17gTlLzXCTtmAd26q+9gYci4qmIWEo6m9kT6JBUSmSbAl15ugvYLO97NWA93OWJmVnD\nVNPlyKnArqSzAyLiTmDLAezzEWB3SWvlGwzfDdwH3EzqzgTgCODaPH1dnicvv8nXN8zMGqeaxLE0\nIp7tUba8vzuMiNmki9x3APNzDOcAJwInSFpIuoZxfn7K+cCGufwE4KT+7tvMzAaumovj90r6CNAm\naRtSU9o/DGSnEXEKcEqP4gdJZzY9130Z+MBA9mfWDKbP62LajAU8tqSbsR3tTJk00eO2W0uq5ozj\neGA74BXgUuA54HP1DMpsuJk+r4upV8+na0k3AXQt6Wbq1fOZPq+rz+eaNZs+E0dEvBQRXyZdi/jX\niPhyPgswsypNm7GA7qXLVinrXrqMaTMG3FDRbMhV06rqbZLmA3eTbgS8S9Iu9Q/NbPh4bEl3TeVm\nzayaqqrzgU9FxISImAAch8ccN6vJ2I72msrNmlk1iWNZRPy/0kxE/A54rcL6ZtbDlEkTaR/dtkpZ\n++g2pkya2KCIzPqvmlZVt0r6MenCeAAfAm6RtDNARNxRx/jMhoVS6ym3qrLhQH3dSyfp5gqLIyL2\nGtyQBq6zszPmzJnT6DDMzFqKpLkR0dnXetX0VfWvgxOSmZkNB9Vc4zAzM1vBicPMzGrixGFmZjWp\n5gbAtSR9RdK5eX4bSe+pf2hmZtaMqjnj+Ampn6o98nwXcHrdIjIzs6ZWTeLYKiK+CSyF1HcVoLpG\nZWZmTauaxPFqHjo2ACRtRToDMTOzEaiaO8dPJQ0bu5mkS0jDvB5Zx5jMzKyJVXMD4A2S5gK7k6qo\nPhsRT9c9MjMza0p9Jg5J1wM/A66LiBfrH5KZmTWzaq5xfAv4F+A+SVdKOkTSmnWOy8zMmlQ1VVW3\nknrIbQP2Aj4GXACsW+fYzMysCVVzcZzcquq9pC7VdwYuqmdQZmbWvKq5xnE5sCupZdXZwK0Rsbze\ngZmZWXOq5ozjfODDEbGs3sGYmVnzK5s4JO0VETcBbwAOkla9WTwiru7vTiV1AOcB25NuLDwaWABc\nBkwAHgY+GBGLlXb8PeAA4CXgSI86aK1o+rwujwBow0KlM453AjeRrm30FEC/EwcpEfwmIg6RtDqw\nFvCfwI0RcZakk4CTgBOB/YFt8mM34If5r1nLmD6vi6lXz6d7aTpx71rSzdSr5wM4eVjLKZs4IuKU\nPPm1iHiouEzSFv3doaT1gHeQ7z6PiFdJ3ZocBLwrr3YRcAspcRwEXBxpjNtZkjokbRIRj/c3BrOh\nNm3GghVJo6R76TKmzVjgxGEtp5r7OK7qpezKAexzC+Ap4CeS5kk6T9IbgI0LyeDvwMZ5ehzwaOH5\ni3LZKiQdK2mOpDlPPfXUAMIzG3yPLemuqdysmVW6xvFmYDtgPUkHFxatCwzkBsDVSE16j4+I2ZK+\nR6qWWiEiQlLUstGIOAc4B6Czs7Om55rV29iOdrp6SRJjO9obEI3ZwFQ645gIvAfoIF3nKD12Jt0E\n2F+LgEURMTvPX5m3+YSkTQDy3yfz8i5gs8LzN81lZi1jyqSJtI9uW6WsfXQbUyZNbFBEZv1X6RrH\ntcC1kvaIiNsHa4cR8XdJj0qaGBELgHcD9+XHEcBZ+e+1+SnXAZ+W9HPSRfFnfX3DWk3pOoZbVdlw\nUM19HPMkHUeqtlpRRRURRw9gv8cDl+QWVQ8CR5HOfi6XdAzwN+CDed1fkZriLiQ1xz1qAPs1a5jJ\nO41zorBhoZrE8b/AX4BJwNeAjwJ/HshOI+JOoLOXRe/uZd0AjhvI/szMbPBU06pq64j4CvBiRFwE\nHIjvozAzG7GqSRxL898lkrYH1gPeWL+QzMysmVVTVXWOpPWBr5AuVK8NfLWuUZmZWdOqZjyO8/Lk\nrcCW9Q3HzMyaXaUbAE+o9MSI+Pbgh2NmZs2u0hnHOkMWhZmZtYxKNwCeNpSBmJlZa+izVZWkN0m6\nUdI9ef4tkk6uf2hmZtaMqmmOey4wldwsNyLuBg6tZ1BmZta8qkkca0XEH3uUvVaPYMzMrPlVkzie\nlrQVadQ/JB0CuJNBM7MRqpobAI8jjXPxZkldwEPAYXWNyszMmlY1NwA+COydR+kbFRHP1z8sMzNr\nVhWrqiS1SdoIICJeBF6R9DFJA+od18zMWlfZxCHpUOAfwN2SbpW0L2nsjANIXaubmdkIVKmq6mRg\nl4hYKGln4HbgkIi4fmhCMzOzZlSpqurViFgIEBF3APc7aZiZWaUzjjf26OiwozjvTg7NzEamSonj\nXFbt6LDnvJmZjUDu5NBsiEyf18W0GQt4bEk3YzvamTJpIpN3GtfosMxqVs0NgGY2QNPndTH16vl0\nL10GQNeSbqZePR/AycNaTjVdjpjZAE2bsWBF0ijpXrqMaTMWNCgis/5z4jAbAo8t6a6p3KyZeehY\nsyEwtqOdrl6SxNiO9gZEYzYwlc441smPTuCTwLj8+ASw80B3nLszmSfpF3l+C0mzJS2UdJmk1XP5\nGnl+YV4+YaD7NhtqUyZNpH102ypl7aPbmDJpYoMiMuu/sokjIk7LLas2BXaOiC9ExBeAXYDxg7Dv\nzwLFPq++AXwnIrYGFgPH5PJjgMW5/Dt5PbOWMnmncZx58A6M62hHwLiOds48eAdfGLeWVE2rqo2B\nVwvzr+ayfpO0KXAgcAZwgiQBewEfyatcBJwK/BA4KE8DXAmcLUkREQOJwWyoTd5pnBOFDQvVJI6L\ngT9KuibPTyZ9sQ/Ed4EvsfKGwg2BJRFRGllwEalajPz3UYCIeE3Ss3n9p4sblHQscCzA+PGDcUJk\nlfiehNqdPH0+l85+lGURtEl8eLfNOH3yDo0Oy6xmfbaqiogzgKNI1UeLgaMi4r/6u0NJ7wGejIi5\n/d1GbyLinIjojIjOMWPGDOamrYfSPQldS7oJVt6TMH1eV6NDa1onT5/PT2c9wrJ8orwsgp/OeoST\np89vcGRmtau2Oe5awHMR8T1gkaQtBrDPPYH3SXoY+Dmpiup7pL6wSmdAmwKlb6EuYDOAvHw94JkB\n7N8GyPck1O7S2Y/WVG7WzPpMHJJOAU4Epuai0cBP+7vDiJgaEZtGxATgUOCmiPgocDNwSF7tCODa\nPH1dnicvv2kkXd+YPq+LPc+6iS1O+iV7nnVTU/yq9z0JtVtW5iNbrtysmVVzxvF+4H3AiwAR8Rj1\n6ezwRNKF8oWkaxjn5/LzgQ1z+QnASXXYd1Nq1iqhcvce+J6E8lRjuVkzqyZxvJp/4QdAHnt8UETE\nLRHxnjz9YETsGhFbR8QHIuKVXP5ynt86L39wsPbf7Jq1Ssj3JNRurdXbaio3a2bVtKq6XNKPSdcg\nPgYcDZxX37Ba22C1OGrWKqHSa3Grquq9+OqymsrNmlmfiSMiviVpH+A5YCLw1YiYWffIWtRg9oLa\nzN1U+J6E2rRJvV7PaJMrq6z1VHNx/BsRMTMipkTEFyNipiTfvV3GYFYvuUpo+PDFcRtOqrnGsU8v\nZfsPdiDDxWBWL7mbiuFjXJmzxHLlZs2sUu+4nwQ+BWwp6e7ConWA39c7sFY12NVLrhIaHqZMmrhK\nFSb47NFaV6Uzjp8B7yXdR/HewmOXiDhsCGJrSa5est747NGGE1V7L52kNwJrluYj4pF6BTVQnZ2d\nMWfOnIbt3/04mVkrkjQ3Ijr7Wq/PVlWS3gt8GxgLPAlsTuoOfbuBBjlcuXrJzIazau7jOB3YHfht\nROwk6V8BV1WNcD6rMhu5qmlVtTQingFGSRoVETeTRgW0EapZu0Ixs6FRTeJYImlt4DbgEknfI/db\nZSNTs3aFYmZDo5rEcRDQDXwe+A3wAKl1lY1QvTU3rlRuZsNLNV2OvAggaV3g+rpHZE1P5B4veyk3\ns+GvmlZVHwdOA14GlrPye2PL+oZmzapcA253nmE2MlTTquqLwPYR8XSfa5qZ2bBXzTWOB4CX6h2I\nmZm1hmrOOKYCf5A0G3ilVBgRn6lbVGZm1rSqSRw/Bm4C5pOucZiZ2QhWTeIYHREn1D0SMzNrCdUk\njl9LOpbUFLdYVfWPukXV4oZ7dxx7brUBv3/g9W//nltt0IBozGyoVXNx/MPk6xzA3PxoXNezTW4k\ndMdxycf2YJs3vmGVsm3e+AYu+dgeDYrIzIZSn4kjIrbo5eF7OMoYCd1xTJ/XxaLFL69Stmjxy8Mq\nOZpZeZVGANwrIm6SdHBvyyPi6vqF1boGc+jYZlUpOQ6nKjkz612lM4535r/v7eXxnv7uUNJmkm6W\ndJ+keyV9NpdvIGmmpPvz3/VzuSR9X9JCSXdL2rm/+x4K5YaI7e/Qsc1oJCRHMyuvbOKIiFPy5Nci\n4qjiA/j6APb5GvCFiNiWNM7HcZK2BU4CboyIbYAb8zzA/sA2+XEs8MMB7LvuRsLQsSMhOZpZedVc\nHL+ql7Ir+7vDiHg8Iu7I08+TRhMcR+qF96K82kXA5Dx9EHBxJLOADkmb9Hf/9TbYY0tPn9fFnmfd\nxBYn/ZI9z7qpKa4jjITkaGblVbrG8WbS8LDr9bjOsS6FsccHQtIEYCdgNrBxRDyeF/0d2DhPjwMe\nLTxtUS61SQ1GAAARJElEQVR7vFBGbjJ8LMD48eMHI7x+u2LOIyu6GO9a0s0Vcx7pV+IotdAqXU8o\ntdACGnotYfJO45jzt39w6exHWRZBm8S/7eLhcs1GikpnHBNJ1zI6WPX6xs7Axwa64zw41FXA5yLi\nueKyiAhq7Gw1Is6JiM6I6BwzZsxAw+u3j557++vucfj9A//go+feXvO2mrWF1vR5XVw1t4tlkd6i\nZRFcNberKc6GzKz+yp5xRMS1wLWS9oiI2r/1KpA0mpQ0Lim0znpC0iYR8Xiuinoyl3cBmxWevmku\na0q93RhXqbySZr0I7VZVZiNbNdc43i9pXUmjJd0o6SlJh/V3h5IEnA/8OSK+XVh0HXBEnj4CuLZQ\nfnhuXbU78GyhSmtY61hrdE3lQ6VZE5qZDY1qEse+uSrpPcDDwNbAlAHsc0/g34G9JN2ZHwcAZwH7\nSLof2DvPA/wKeBBYCJwLfGoA+24pUaayrlz5UHGrKrORrapODvPfA4ErIuLZdNLQPxHxO8qPMvru\nXtYP4Lh+73CIjaL3LoSrydA9Pdu9tKbyoTJl0kSmXHkXS5etzGCj2+RWVWYjRDXfZ9dL+guwC3Cj\npDGkYWStF+u2916NVK68kvXKPKdc+VBatjwqzpvZ8NXnGUdEnCTpm6RrC8skvUS6t8J6MZhnCeVO\n7Cqd8J08ff4qzWQ/vNtmnD55h5r3Xclp199LzzyxPFK5L46bDX9lzzgkfakw++6IWAYQES8CHv2v\njMGs/1/yUu/Jplz5ydPn89NZj6zSTPansx7h5Onza953JYvL7L9cuZkNL5Wqqg4tTE/tsWy/OsQy\nLAzmXdW1JKHp87r46axHel3/0tmP9lpuZtYflRKHykz3Nm/ZYHY5MmXSxF4PfM8kVLrDvJxljW6G\nZWbDSqXEEWWme5u3OvjBzff3euB/cPP9q5T1dkNeUdsAWsGZmfVU6eL4WyU9R/qR256nyfOD0lfV\ncDR9XhdTrriLpfnqcdeSbqZccRdQe/9S9z/5YlXlfd149+HdNqu43MysFpW6HGkrt8yS3low/eKu\nx1ckjZKly4NTr6tfi6OxHe0rOlXs6bDdxw96qyozG9n6c1+aUb4F05IyzW7LlQ+Gchfkv/uhHZ00\nzGzQOXH0UzO1VBrsMUD6Mq5Ma69y5WY2vFTT5Yj1otaWSmusVt8cPXmnoRsPY8qkiauMEwIeyMls\nJHHi6Kc2qabk8cpry4fkru6hUEpQ02Ys4LEl3YztaGfKpIm+a9xshHDiqFHxy79WxRv0StdEgJZN\nHk4UZiOTE0cF0+d1rfKresKG7WUHZCqdQZS7e7ucS2c/2pKJw8xGLieOMnob77tck1cBD5x5AEDN\nicN3dZtZq3GrqjJOve7eindjFw30q3+Lk37Jnmfd5DG7zawlOHH0Yvq8rrred9FTkM5oPn/ZnYPe\nk62Z2WBz4ujFtBkL6rLd9tGjGFWh26ggVXWVzjzKreqep8yskZw4etjtjJllr2VUUqpuqqR76XLW\nWC3d0V3py/+06+8FyleB+aqImTWSE0fBW075DU88/2q/nluqbupL99JlTJuxoOLATh4QycyamRNH\nNn1eF8+9Ut3F8IF6bEl3VXdZl+sO3d2km1kjOXFk9bqu0ZuxHe1M3mkc7aN7P/wd7aOB8t2hu5t0\nM2skJ46sP9c1+qPYp9OZB7+F0T2ulo8eJU5933ZAuqP8sN3HrzjDaJPcTbqZNVzL3AAoaT/ge0Ab\ncF5EnNXgkGo2rkefTtX0+XT65B2cKMysqbRE4pDUBvwA2AdYBPxJ0nURcV9jI1tJpCqo57tf7fVa\nycbrrM7vT9rrdeXu88nMWk1LJA5gV2BhRDwIIOnnwEHAoCWOWnu7LRLw0FkHrpjf7YyZq7TO2nid\n1Zn95X0GGqKZWVNolcQxDiiOnLQI2K24gqRjgWMBxo8fX/MO+tNBIbw+aQBOEmY2rLVK4uhTRJwD\nnAPQ2dlZ86lD6TpCcbyM3bdcn4ef6faYE2ZmBa2SOLqAYhvUTXPZoPKFaDOzvrVKc9w/AdtI2kLS\n6sChwHUNjsnMbERqiTOOiHhN0qeBGaTmuBdExL0NDsvMbERqicQBEBG/An7V6DjMzEa6VqmqMjOz\nJuHEYWZmNVEMwzGvJT0F/K2fT98IeHoQwxlqrRx/K8cOjr+RWjl2aJ74N4+IMX2tNCwTx0BImhMR\nnY2Oo79aOf5Wjh0cfyO1cuzQevG7qsrMzGrixGFmZjVx4ni9cxodwAC1cvytHDs4/kZq5dihxeL3\nNQ4zM6uJzzjMzKwmThxmZlYTJ44CSftJWiBpoaSTGh1PiaSHJc2XdKekOblsA0kzJd2f/66fyyXp\n+/k13C1p58J2jsjr3y/piDrGe4GkJyXdUygbtHgl7ZKPx8L83FUHbh/82E+V1JWP/52SDigsm5rj\nWCBpUqG8189S7qhzdi6/LHfaOWgkbSbpZkn3SbpX0mdzedMf/wqxt8Txl7SmpD9KuivHf1qlfUpa\nI88vzMsn9Pd1DbmI8CNd52kDHgC2BFYH7gK2bXRcObaHgY16lH0TOClPnwR8I08fAPyaNMbU7sDs\nXL4B8GD+u36eXr9O8b4D2Bm4px7xAn/M6yo/d/86x34q8MVe1t02f07WALbIn5+2Sp8l4HLg0Dz9\nI+CTg3zsNwF2ztPrAH/NcTb98a8Qe0sc/3w81s7To4HZ+Tj1uk/gU8CP8vShwGX9fV1D/fAZx0or\nhqeNiFeB0vC0zeog4KI8fREwuVB+cSSzgA5JmwCTgJkR8Y+IWAzMBParR2ARcRvwj3rEm5etGxGz\nIv2XXVzYVr1iL+cg4OcR8UpEPAQsJH2Oev0s5V/mewFX5ucXj8Ngxf94RNyRp58H/kwaQbPpj3+F\n2MtpquOfj+ELeXZ0fkSFfRbfkyuBd+cYa3pdgxV/LZw4VupteNpmGe4vgBskzVUaIhdg44h4PE//\nHdg4T5d7HY1+fYMV77g83bO83j6dq3IuKFXz9BFjb+UbAksi4rUe5XWRqz52Iv3ybanj3yN2aJHj\nL6lN0p3Ak6Rk+0CFfa6IMy9/NsfYrP/DKzhxtIa3R8TOwP7AcZLeUVyYf/m1TLvqVosX+CGwFbAj\n8Djw340Np2+S1gauAj4XEc8VlzX78e8l9pY5/hGxLCJ2JI1Suivw5gaHVBdOHCsNyfC0/RERXfnv\nk8A1pA/kE7nagPz3ybx6udfR6Nc3WPF25eme5XUTEU/kL4TlwLmk408fMfZW/gypKmi1HuWDStJo\n0hfvJRFxdS5uiePfW+ytdvxzzEuAm4E9KuxzRZx5+Xo5xmb9H17BiWOlphyeVtIbJK1Tmgb2Be4h\nxVZq6XIEcG2evg44PLeW2R14NldRzAD2lbR+PtXfN5cNlUGJNy97TtLuuT748MK26qL0hZu9n3T8\nS7EfmlvHbAFsQ7pw3OtnKf/Svxk4JD+/eBwGK1YB5wN/johvFxY1/fEvF3urHH9JYyR15Ol2YB/S\ndZpy+yy+J4cAN+UYa3pdgxV/TRpxRb5ZH6QWJn8l1Ut+udHx5Ji2JLWeuAu4txQXqS70RuB+4LfA\nBrlcwA/ya5gPdBa2dTTpQttC4Kg6xnwpqUphKake9pjBjBfoJH15PACcTe4BoY6x/2+O7W7SP+om\nhfW/nONYQKF1UbnPUn4//5hf0xXAGoN87N9Oqoa6G7gzPw5oheNfIfaWOP7AW4B5Oc57gK9W2iew\nZp5fmJdv2d/XNdQPdzliZmY1cVWVmZnVxInDzMxq4sRhZmY1ceIwM7OaOHGYmVlNnDisJpJC0k8L\n86tJekrSL/q5vYclbdRL+fsGq/dPSWtL+rGkB3K3LbdI2q0f27lQ0iF9rwmSvqyVvbkuK0x/RtIn\nJB1e+yupar9XStoyT/+qdF9Bq8jvTWeF5d+StNdQxmSvt1rfq5it4kVge0ntEdFNuslp0O9ejYjr\nGLybm84DHgK2iYjl+aaqbQdp272KiDOAMwAkvRCpG4q6krQd0BYRD+YYDujjKa3of0h3j9/U6EBG\nMp9xWH/8CjgwT3+YdNMcAJJ2lXS7pHmS/iBpYi5vy78W78md1R1f2N7xku5QGuPhzXn9IyWdnacv\nVBr34Q+SHiz+6pc0RdKf8jZP6xmopK2A3YCTI3VZQUQ8FBG/lPQ1SZ8rrHuGVo4BcWKO5y5JZ/Wy\n3V0k3ZrPYGb0uLu5IqXxJb6Yp2+R9B1JcyT9WdLbJF2tNAbG6YXnHKY01sOd+eyprZdNf5TCndCl\nszlJE/K2z1UaJ+KGfGdzz7g+kN+fuyTdlsvaJE0rHOOPF9Z/3TGStKOkWXnda7Ry3I9bJH0jv4a/\nSvqXXN4u6ec5vmuA9sJ+L8zxzJf0+fze/Q3YUNI/VXu8rQ4adeehH635AF4g3SF7JenO1zuBdwG/\nyMvXBVbL03sDV+XpT+bnlJaV7lx+GDg+T38KOC9PHwmcnacvJN1hO4p0prAwl+8LnEO6+3kU8Avg\nHT3ifR9wTZnXMgG4I0+PIt2NuyGpM8k/AGv1iPVCUtcQo/PyMbn8Q8AFlY5Zj/lTyeNLALewcmyM\nzwKPkcalWIN05/qGwD8D1wOj83r/Fzi8l/3cCuxQmH8Y2Ci/zteAHXP55cBhvTx/PjAuT3fkv8eS\nki45pjmkMSLKHaO7gXfm6a8B3y28zv/O0wcAv83TJ5SOHelz9RrpzvRdSN26U4wnT58L/Fuj/xdG\n8sNVVVaziLhbqdvrD5POPorWAy6StA2p+4jRuXxv0qA1r+VtFMe8KHXENxc4uMxup0c6Y7hPUqlL\n8H3zY16eX5vUr89tVb6OhyU9I2knUjfj8yLiGUl7Az+JiJd6iRVgIrA9MFNp8Ls2Ujcl/VWqkpsP\n3Bu5+3NJD5I6tXs76Yv0T3l/7azspLBoE+CpMvt4KCLuzNNzScmkp98DF0q6nJXvyb7AWwpneeuR\njvHrjpGk9Uhf8LfmdS8iJfyS4vtc2v87gO/nbdwt6e5c/iCwpaT/AX4J3FDYzpPA2DKv04aAE4f1\n13XAt0hnGxsWyr8O3BwR78/J5ZYqtvVK/ruM8p/JVwrTKvw9MyJ+XGHb9wJvldQWEct6WX4e6ezm\nn4ALqoi1tN97I2KPKtfvS+m1LWfV17mcdDwEXBQRU/vYTjfpLLDSPiAd59dVVUXEJ5QaDRwIzJW0\nS9738RGxSoeYKgxnWoNq3udSLIslvZU0oNQngA+S+s6C9Bq7+7F/GyS+xmH9dQFwWkTM71G+Hisv\nlh9ZKJ8JfFy5e2lJGwxCDDOAo5XGb0DSOElvLK4QEQ+QqldOU/65nuv8S9doriGNhPg2VvYWPBM4\nStJaZWJdAIyRtEdePlrpwnS93AgcUnptSuOHb97Len8Gtu7vTiRtFRGzI+KrpDOXzUjH5JNK3Z0j\n6U1KvTS/7hhFxLPA4tL1C+DfSdVnldwGfCRvY3tSdRVKLe1GRcRVwMmk4XxL3sTKHnKtAXzGYf0S\nEYvIVQw9fJNUVXUyqYqh5DzSP/zdkpaS6qnPHmAMN0j6Z+D2nBNeAA7j9dU4/0Ea/GehpG7gaWBK\n3sarkm4mjdK2LJf9RtKOwBxJr5Kq4/6zsN9Xc9XN93P1zGrAd0lnN4MuIu7Lx/MGSaNIPfceB/yt\nx6q/JJ0B/rafu5qWqxhFSlZ3ka5ZTADuyIn3KWByhWN0BPCjnFAeBI7qY58/BH4i6c+kxDc3l4/L\n5aUft1NhxXgdW5N+DFiDuHdcG9HyF9MdwAci4v5GxzMQuaXUzcCeZarlWp6k9wM7R8RXGh3LSOaq\nKhuxJG1LGgvhxlZPGgCR7qs5hQaNQz1EVqOJh44dKXzGYWZmNfEZh5mZ1cSJw8zMauLEYWZmNXHi\nMDOzmjhxmJlZTf4/zh4cndKhIW8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f833ac42e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(L[3],L[9])\n",
    "plt.xlabel('Machine Cycle Time (in seconds)')\n",
    "plt.ylabel('Estimated Relative performance')\n",
    "plt.title('Comparison between MCT and performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>206.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>206.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.597087</td>\n",
       "      <td>204.849515</td>\n",
       "      <td>2896.310680</td>\n",
       "      <td>11880.563107</td>\n",
       "      <td>24.330097</td>\n",
       "      <td>4.640777</td>\n",
       "      <td>17.223301</td>\n",
       "      <td>105.800971</td>\n",
       "      <td>99.451456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.055030</td>\n",
       "      <td>262.015754</td>\n",
       "      <td>3898.714252</td>\n",
       "      <td>11789.268990</td>\n",
       "      <td>37.523947</td>\n",
       "      <td>6.813694</td>\n",
       "      <td>23.813680</td>\n",
       "      <td>161.745100</td>\n",
       "      <td>155.607559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>45.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>16000.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>100.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>64000.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>1150.000000</td>\n",
       "      <td>1238.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            2             3             4           5  \\\n",
       "count  206.000000   206.000000    206.000000    206.000000  206.000000   \n",
       "mean    15.597087   204.849515   2896.310680  11880.563107   24.330097   \n",
       "std      7.055030   262.015754   3898.714252  11789.268990   37.523947   \n",
       "min      1.000000    17.000000     64.000000     64.000000    0.000000   \n",
       "25%     10.000000    50.000000    768.000000   4000.000000    0.000000   \n",
       "50%     17.000000   110.000000   2000.000000   8000.000000    8.000000   \n",
       "75%     20.000000   225.000000   4000.000000  16000.000000   32.000000   \n",
       "max     27.000000  1500.000000  32000.000000  64000.000000  256.000000   \n",
       "\n",
       "                6           7            8            9  \n",
       "count  206.000000  206.000000   206.000000   206.000000  \n",
       "mean     4.640777   17.223301   105.800971    99.451456  \n",
       "std      6.813694   23.813680   161.745100   155.607559  \n",
       "min      0.000000    0.000000     6.000000    15.000000  \n",
       "25%      1.000000    5.000000    27.000000    28.000000  \n",
       "50%      2.000000    8.000000    49.500000    45.500000  \n",
       "75%      6.000000   24.000000   112.500000   100.500000  \n",
       "max     52.000000  176.000000  1150.000000  1238.000000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " Right off the back, we notice that the features are differing too much in magnitude. This might cause a hinderance when converging the gradient descent. Hence we want to scale these features so they are closer to each other. Hence, one way to do this is to subtract by the mean and divide it by either the range or the standard deviation. The process is termed as 'Normalisation'. Refer to Andrew Ng coursera video for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, in the below normalisation, I also store the mean and standard deviation of every column since when we predict, we will input the unaltered values and hence, have to normalise it with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.671141</td>\n",
       "      <td>1.309070</td>\n",
       "      <td>1.706589</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.493011</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>1.008989</td>\n",
       "      <td>0.986768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.671141</td>\n",
       "      <td>1.309070</td>\n",
       "      <td>1.706589</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.493011</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>0.706043</td>\n",
       "      <td>0.986768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.671141</td>\n",
       "      <td>1.309070</td>\n",
       "      <td>1.706589</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.493011</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>0.409280</td>\n",
       "      <td>0.986768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.671141</td>\n",
       "      <td>1.309070</td>\n",
       "      <td>0.349423</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.493011</td>\n",
       "      <td>-0.051370</td>\n",
       "      <td>0.161977</td>\n",
       "      <td>0.209171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.682591</td>\n",
       "      <td>1.309070</td>\n",
       "      <td>1.706589</td>\n",
       "      <td>1.057189</td>\n",
       "      <td>0.493011</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>1.311935</td>\n",
       "      <td>1.224546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.694040</td>\n",
       "      <td>3.361028</td>\n",
       "      <td>1.706589</td>\n",
       "      <td>1.057189</td>\n",
       "      <td>1.667117</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>1.614881</td>\n",
       "      <td>1.809350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.694040</td>\n",
       "      <td>3.361028</td>\n",
       "      <td>1.706589</td>\n",
       "      <td>1.057189</td>\n",
       "      <td>1.667117</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>2.369154</td>\n",
       "      <td>1.809350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.694040</td>\n",
       "      <td>3.361028</td>\n",
       "      <td>4.420922</td>\n",
       "      <td>1.057189</td>\n",
       "      <td>1.667117</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>3.277991</td>\n",
       "      <td>4.174274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.694040</td>\n",
       "      <td>7.464945</td>\n",
       "      <td>4.420922</td>\n",
       "      <td>2.762766</td>\n",
       "      <td>4.015329</td>\n",
       "      <td>1.964278</td>\n",
       "      <td>6.418736</td>\n",
       "      <td>7.316795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0.744804</td>\n",
       "      <td>-0.486394</td>\n",
       "      <td>-0.753275</td>\n",
       "      <td>-0.648389</td>\n",
       "      <td>-0.534332</td>\n",
       "      <td>-0.639267</td>\n",
       "      <td>-0.419184</td>\n",
       "      <td>-0.491309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.744804</td>\n",
       "      <td>-0.611563</td>\n",
       "      <td>-0.710864</td>\n",
       "      <td>-0.541790</td>\n",
       "      <td>-0.534332</td>\n",
       "      <td>-0.471296</td>\n",
       "      <td>-0.406819</td>\n",
       "      <td>-0.484883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.552828</td>\n",
       "      <td>-0.229899</td>\n",
       "      <td>-0.329161</td>\n",
       "      <td>1.083839</td>\n",
       "      <td>-0.534332</td>\n",
       "      <td>-0.387311</td>\n",
       "      <td>-0.085325</td>\n",
       "      <td>-0.189268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.590993</td>\n",
       "      <td>0.283091</td>\n",
       "      <td>0.349423</td>\n",
       "      <td>1.083839</td>\n",
       "      <td>-0.534332</td>\n",
       "      <td>-0.387311</td>\n",
       "      <td>0.199073</td>\n",
       "      <td>0.112774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>0.553976</td>\n",
       "      <td>-0.726473</td>\n",
       "      <td>-1.002315</td>\n",
       "      <td>-0.648389</td>\n",
       "      <td>-0.534332</td>\n",
       "      <td>-0.555282</td>\n",
       "      <td>-0.592296</td>\n",
       "      <td>-0.542721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.018508</td>\n",
       "      <td>-0.611563</td>\n",
       "      <td>0.349423</td>\n",
       "      <td>-0.648389</td>\n",
       "      <td>-0.094042</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>-0.437732</td>\n",
       "      <td>-0.227826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.144455</td>\n",
       "      <td>-0.608485</td>\n",
       "      <td>-0.838098</td>\n",
       "      <td>-0.435191</td>\n",
       "      <td>-0.094042</td>\n",
       "      <td>-0.093362</td>\n",
       "      <td>-0.536653</td>\n",
       "      <td>-0.491309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.236053</td>\n",
       "      <td>-0.611563</td>\n",
       "      <td>-0.583629</td>\n",
       "      <td>-0.648389</td>\n",
       "      <td>0.346247</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>-0.481010</td>\n",
       "      <td>-0.452751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.236053</td>\n",
       "      <td>-0.486394</td>\n",
       "      <td>-0.838098</td>\n",
       "      <td>-0.648389</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>-0.051370</td>\n",
       "      <td>-0.462462</td>\n",
       "      <td>-0.497736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.361999</td>\n",
       "      <td>0.539585</td>\n",
       "      <td>-0.583629</td>\n",
       "      <td>3.135862</td>\n",
       "      <td>0.493011</td>\n",
       "      <td>1.964278</td>\n",
       "      <td>0.087786</td>\n",
       "      <td>0.157759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.236053</td>\n",
       "      <td>-0.358146</td>\n",
       "      <td>-0.473360</td>\n",
       "      <td>-0.648389</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>-0.468645</td>\n",
       "      <td>-0.414192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         2         3         4         5         6         7         8  \\\n",
       "1   1 -0.671141  1.309070  1.706589  0.204400  0.493011  0.620513  1.008989   \n",
       "2   1 -0.671141  1.309070  1.706589  0.204400  0.493011  0.620513  0.706043   \n",
       "3   1 -0.671141  1.309070  1.706589  0.204400  0.493011  0.620513  0.409280   \n",
       "4   1 -0.671141  1.309070  0.349423  0.204400  0.493011 -0.051370  0.161977   \n",
       "5   1 -0.682591  1.309070  1.706589  1.057189  0.493011  0.620513  1.311935   \n",
       "6   1 -0.694040  3.361028  1.706589  1.057189  1.667117  0.620513  1.614881   \n",
       "7   1 -0.694040  3.361028  1.706589  1.057189  1.667117  0.620513  2.369154   \n",
       "8   1 -0.694040  3.361028  4.420922  1.057189  1.667117  0.620513  3.277991   \n",
       "9   1 -0.694040  7.464945  4.420922  2.762766  4.015329  1.964278  6.418736   \n",
       "10  2  0.744804 -0.486394 -0.753275 -0.648389 -0.534332 -0.639267 -0.419184   \n",
       "11  2  0.744804 -0.611563 -0.710864 -0.541790 -0.534332 -0.471296 -0.406819   \n",
       "12  3 -0.552828 -0.229899 -0.329161  1.083839 -0.534332 -0.387311 -0.085325   \n",
       "13  3 -0.590993  0.283091  0.349423  1.083839 -0.534332 -0.387311  0.199073   \n",
       "14  4  0.553976 -0.726473 -1.002315 -0.648389 -0.534332 -0.555282 -0.592296   \n",
       "15  4 -0.018508 -0.611563  0.349423 -0.648389 -0.094042  0.620513 -0.437732   \n",
       "16  5 -0.144455 -0.608485 -0.838098 -0.435191 -0.094042 -0.093362 -0.536653   \n",
       "17  5 -0.236053 -0.611563 -0.583629 -0.648389  0.346247  0.620513 -0.481010   \n",
       "18  5 -0.236053 -0.486394 -0.838098 -0.648389  0.052721 -0.051370 -0.462462   \n",
       "19  5 -0.361999  0.539585 -0.583629  3.135862  0.493011  1.964278  0.087786   \n",
       "20  5 -0.236053 -0.358146 -0.473360 -0.648389  0.052721  0.620513 -0.468645   \n",
       "\n",
       "           9  \n",
       "1   0.986768  \n",
       "2   0.986768  \n",
       "3   0.986768  \n",
       "4   0.209171  \n",
       "5   1.224546  \n",
       "6   1.809350  \n",
       "7   1.809350  \n",
       "8   4.174274  \n",
       "9   7.316795  \n",
       "10 -0.491309  \n",
       "11 -0.484883  \n",
       "12 -0.189268  \n",
       "13  0.112774  \n",
       "14 -0.542721  \n",
       "15 -0.227826  \n",
       "16 -0.491309  \n",
       "17 -0.452751  \n",
       "18 -0.497736  \n",
       "19  0.157759  \n",
       "20 -0.414192  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalise(X):\n",
    "    mean=X.mean()\n",
    "    sd=X.std()\n",
    "    \n",
    "    for x in X:\n",
    "        rep=(x-mean)/sd\n",
    "        X.replace(x,rep,inplace=True)\n",
    "    return mean,sd\n",
    "\n",
    "mean_r=[]\n",
    "std_r=[]\n",
    "        \n",
    "for i in range(0,10):\n",
    "    if(i!=0 and i!=1):\n",
    "        m,s=normalise(L[i])\n",
    "        mean_r.append(m)\n",
    "        std_r.append(s)\n",
    "L.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Much better. All the columns are in range of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we have processed the data. For simplicity sake, I have also not considered the company type since if you draw the scatter plot of the target with X[0], you will notice that there is no good straight line fit to it and it produces too much variance. Notice below the command **as_matrix** converts our data frame to a numpy N dimensional array with outermost elements being the rows of the data frame. From there, I have split the data into X independants and y dependant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.671141  ,  1.30906986,  1.70658901, ...,  0.49301059,\n",
       "         0.62051304,  1.00898902],\n",
       "       [-0.671141  ,  1.30906986,  1.70658901, ...,  0.49301059,\n",
       "         0.62051304,  0.70604321],\n",
       "       [-0.671141  ,  1.30906986,  1.70658901, ...,  0.49301059,\n",
       "         0.62051304,  0.40927997],\n",
       "       ..., \n",
       "       [-0.40779805, -0.48639386, -0.32916062, ..., -0.38756902,\n",
       "        -0.38731103, -0.36972354],\n",
       "       [ 1.05012955, -0.61156333, -0.32916062, ..., -0.68109555,\n",
       "        -0.72325239, -0.23988962],\n",
       "       [ 1.05012955, -0.48639386, -0.66845223, ..., -0.68109555,\n",
       "        -0.72325239, -0.37590611]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat=L.as_matrix()\n",
    "X=mat[:,1:8]\n",
    "y=mat[:,8]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "However, we aren't done just yet. If you remember, the multiple regression equation has a bias term. A term that has weight with an input of 1. Hence, we must add that as well to the independants list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.671141  ,  1.30906986, ...,  0.49301059,\n",
       "         0.62051304,  1.00898902],\n",
       "       [ 1.        , -0.671141  ,  1.30906986, ...,  0.49301059,\n",
       "         0.62051304,  0.70604321],\n",
       "       [ 1.        , -0.671141  ,  1.30906986, ...,  0.49301059,\n",
       "         0.62051304,  0.40927997],\n",
       "       ..., \n",
       "       [ 1.        , -0.40779805, -0.48639386, ..., -0.38756902,\n",
       "        -0.38731103, -0.36972354],\n",
       "       [ 1.        ,  1.05012955, -0.61156333, ..., -0.68109555,\n",
       "        -0.72325239, -0.23988962],\n",
       "       [ 1.        ,  1.05012955, -0.48639386, ..., -0.68109555,\n",
       "        -0.72325239, -0.37590611]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=y.size\n",
    "it=ones(shape=(m,8))\n",
    "it[:,1:8]=X\n",
    "it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Awesome. So our input X is ready and so is y. We can begin writing our gradient descent code. Refer to Andrew Ng's video on the equations. This is split into 2 parts:\n",
    "1. The cost function\n",
    "    -- We first write the function to compute the cost function with parameters as the theta matrix\n",
    "2. The gradient descent\n",
    "    -- The gradient descent is run on the cost function and theta is updated from here and this is continued for a set number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "theta=zeros(shape=(8,1))\n",
    "iters=1500\n",
    "alpha=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X,y,theta):\n",
    "    predictions=X.dot(theta)\n",
    "    sqErrors=predictions-y\n",
    "    J=(1.0/(2*m))*sqErrors.T.dot(sqErrors)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Above, notice that multiplying every theta with its corresponding x value can be done by just taking the dot product, also known as Scalar outer product. The squared error, can also be expressed as the outer product between the error matrix and its transpose. Note also that J and predictions will be numbers. Again, Andrew Ng explains this much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,theta,alpha,iters):\n",
    "    J_history=zeros(shape=(iters,1))\n",
    "    \n",
    "    for i in range(iters):\n",
    "        predictions=X.dot(theta)\n",
    "        theta_size=theta.size\n",
    "        \n",
    "        for it in range(theta_size):\n",
    "            temp=X[:,it]\n",
    "            temp.shape=(m,1)\n",
    "            \n",
    "            errors_x1=(predictions-y)*temp\n",
    "            theta[it][0]=theta[it][0]-alpha*(1.0/m)*errors_x1.sum()\n",
    "        J_history[i, 0]=compute_cost(X,y,theta)\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above code cannot be explained simply but read it once while looking at the formula and you will get the meaning of it.\n",
    "Now that both functions have been written, all we have to do is call the function for our dataset. So, lets do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.88390342e-17]\n",
      " [  7.09198075e-02]\n",
      " [  1.77180956e-01]\n",
      " [  2.78126424e-01]\n",
      " [  3.71114602e-02]\n",
      " [ -7.43748097e-03]\n",
      " [  8.20852919e-02]\n",
      " [  5.23279884e-01]] [[ 0.46096208]\n",
      " [ 0.42737845]\n",
      " [ 0.39656888]\n",
      " ..., \n",
      " [ 0.02057334]\n",
      " [ 0.02057266]\n",
      " [ 0.02057198]]\n"
     ]
    }
   ],
   "source": [
    "y.shape=(m,1)\n",
    "theta,J_history=gradient_descent(it,y,theta,alpha,iters)\n",
    "print theta,J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Perfect. Our algorithm has given some parameters and observing J_history, we can see that it is decreasing as the iterations increase. However, let us visualise a few more things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkHrJPZefMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f833e4f0d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(arange(iters),J_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This looks like a proper gradient descent algorithm. Running more iterations will further reduce the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Okay, so all this is well and good. But we are missing an important aspect: how accurate or reliable is our model? We can't use a new input since we won't know whether it is giving the right output. But what we can do is check against the values we have. But we can;t use a value that is already in the training set. This is due to overfitting. Sometimes, a trained model will end up fitting the training values rather than fir a general pattern. The end result is that it will predict properly only within the training set accurately but fail sometimes on external examples. We thus, split our X and y into training and testing sets and use the test set to predict accuracy.\n",
    "\n",
    "Scikit learn comes with a split function that allows us to decide the ratio of data between the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00531171]\n",
      " [ 0.07282308]\n",
      " [ 0.14457074]\n",
      " [ 0.28665628]\n",
      " [ 0.03262975]\n",
      " [-0.00519909]\n",
      " [ 0.06515479]\n",
      " [ 0.57042189]] [[ 0.02369021]\n",
      " [ 0.02368332]\n",
      " [ 0.02367697]\n",
      " ..., \n",
      " [ 0.02328851]\n",
      " [ 0.02328848]\n",
      " [ 0.02328845]]\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(it,y,test_size=0.2)\n",
    "\n",
    "m=y_train.size\n",
    "theta,J_history=gradient_descent(X_train,y_train,theta,alpha,iters)\n",
    "print theta,J_history\n",
    "#plt.plot(arange(iters),J_history)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10661153448317082"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict=X_test.dot(theta)\n",
    "errors=y_predict-y_test\n",
    "ab=np.absolute(errors)\n",
    "mean_error=(ab.sum()/errors.size)\n",
    "mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, our model has a mean squared error of about 3% and we can assume that our algorithm is about 97% accurate. Note that scikit learn comes with many accuracy score methods but cannot be used here since we haven't explicitly used the regression model present there. As a next step, let us predict what would be the accuracy of fitting this data into the inbuilt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.093714823662668006"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "err=mean_absolute_error(y_test,y_pred)\n",
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So, we can see more or less, the mean absolute errors is close by and in some cases, the gradient descent code written can have better accuracy. Increasing the number of iterations beyond 10000 will yield better resukts but always be careful as too many iterations result in over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the code has been rather big and we have many things to consider for Graidient Descent like feature scaling, local minimas etc. However, a rather fast method exists in the normal equation method. So long as our number of training samples is less, we can use this to find theta values in one step equation without having to feature scale, set the learning rate or iterate many times. Roughly, the equation is (X'X)^-1.X'.y\n",
    "    Refere to Andrew Ng's videos aagain. Here, X is called the design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.08166817e-17],\n",
       "       [  6.28810173e-02],\n",
       "       [  1.39750595e-01],\n",
       "       [  2.55771174e-01],\n",
       "       [  2.29992137e-02],\n",
       "       [  4.15653344e-05],\n",
       "       [  5.39111142e-02],\n",
       "       [  6.00857169e-01]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_des=it\n",
    "A=X_des.T.dot(X_des)\n",
    "B=np.linalg.inv(A).dot(X_des.T)\n",
    "theta_n=B.dot(y)\n",
    "theta_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when compared with the theta values obtained from the Gradient Descent algorithm, the values are almost similar with errors of 0.5% or less. So, it is a close enough approximation. Again, one thing to keep in mind is that X'X is an n square matrix where n is number of features. Thus, calculating its inverse is very cumbersome when no. of features becomes too large. Thus, when n>1000 etc. it is better to use Gradient Descent else the normal equation gives satisfactory results very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you've made it this far, congrats on writing your very own Gradient Descent algorithm. Although scikit learn has its own model which we can implement in 4 lines, understanding how gradient descent works will form the basis on how Stochastic Gradient descent, addition of momentum etc. affects the model. I hope you understood this notebook. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
